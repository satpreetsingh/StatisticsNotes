# Basic facts



## Common GLM distributions

\begin{figure}[!htbp]
\includegraphics{GLMdistributions.pdf}
\end{figure}

Given the pdf:

\begin{equation}
f(y; \theta_i, \phi)= \exp[\frac{y\theta_i - b(\theta_i)}{\phi/w}+c(y,\phi)]
\end{equation}

We know that 

\begin{equation}
E(Y_i)=\mu_i = h(x_i^T \beta) = b'(\theta)
\end{equation}


Therefore:

\begin{equation}
x_i^T \beta =  h^{-1}(b'(\theta)) = \explain{g}{\hbox{canonical link}}b'(\theta)
\end{equation}


\begin{tabular}{|l|l|l|}
\hline
Distribution & $h(x_i^T \beta)=\mu_i$ & $g(\mu_i)=\theta_i$\\
\hline
Binomial & $\frac{\exp[\theta_i]}{1+\exp[\theta_i]}$ & $\log \frac{y}{1-y}$ \\
logit link & & \\
\hline
Normal & $\theta$ & $g=h$ \\
identity & & \\
\hline
Poisson & $\exp[\theta]$ & $\log[\mu]$ \\
log & & \\
\hline
Gamma & $-\frac{1}{\theta}$ & $-\frac{1}{\mu_i}$\\
inverse & & \\
\hline
Cloglog & $1-\exp[-\exp[\theta_i]]$ & $\log(-\log(1-\mu_i))$\\
cloglog & & \\
\hline
Probit & $\Phi(\theta)$ & 
$\Phi^{-1}(\theta)$ (qnorm)\\
probit & & \\
\hline
\end{tabular}

\medskip
The big thing about the canonical link is that is expresses $\theta_i$ as a linear combination of the parameters: $x_i^T \beta$.

**Relevance of canonical link**: You can decide which link to use by plotting $g(\mu_i)$ against the predictor (in case we have only a single predictor x).



# Iteratively reweighted least squares


\begin{enumerate}
\item Specify an **initial vector of parameters**: $b^{(m)}= (\beta_0,\dots,\beta_p)^T$, where initially $m=1$:



```{r echo=F}
load("data/MAS473.RData")
```


```{r}
## eta=xbeta:
eta.i<- -60+35*beetle$conc
```



\item **Specify a weight matrix W** that depends on current parameter estimates:

Given (proof on p.\  83-84):

\begin{equation}
  w_{ii} =\frac{n_i \exp[\eta_i]}{(1+\exp[\eta_i])^2}
\end{equation}

we can compute W:

```{r}
n.i <- beetle$number
w.ii.fn<-function(n.i,eta.i){
  (n.i*exp(eta.i))/(1+exp(eta.i))^2
}
w.iis<-w.ii.fn(n.i,eta.i)
##weights matrix:
W<-diag(as.vector(w.iis))
```

\item
**Specify a vector z** that depends on the current parameter estimates and response values:

\begin{equation}
z_i = \eta_i + \frac{y_i - \mu_i}{\mu_i (1-\mu_i)} 
\quad \mu_i = \frac{exp[\eta_i]}{1+exp[\eta_i]}
\end{equation}

```{r}
mu.i<-exp(eta.i)/(1+exp(eta.i))
z.i<-eta.i + ((beetle$propn.dead-mu.i))/
              (mu.i*(1-mu.i))
```


\item
Compute new estimate of parameters: $b^{(m+1)}=(X^T W X)^{-1} X^T W z$:




```{r}
##The design matrix:
col1<-c(rep(1,8))
X<-as.matrix(cbind(col1,beetle$conc))
## update coefs:
eta.i<-solve(t(X)%*%W%*%X)%*%
               t(X)%*%W%*%z.i
```

Stop at convergence. 
\end{enumerate}


# Residual deviances

\begin{enumerate}
\item Normal: $\sum (y_i - \hat{\mu}_i)^2$
\item Poisson: $2\sum y_i 
\log(\frac{y_i}{\hat{\mu}_i}) - (y_i - \hat{\mu}_i$
\item Binomial: 

\begin{equation*}
-2 \sum_i n_i [y_i\log(\frac{ \hat{\mu}_i}{y_i})+ (1-y_i)\log(\frac{1-\hat{\mu}_i}{1-y_i})]
\end{equation*}


\item Gamma: $-2\sum \log(\frac{ y_i}{\hat{\mu}_i}) - \frac{ y_i-\hat{\mu}_i}{\hat{\mu}_i}$
\end{enumerate}

# Testing model fit using pseudo $R^2$ and GLRT

Let the log likelihood of the minimal model (with only an intercept) be:

\begin{equation}
l(\tilde{\mu}, \phi; y)
\end{equation}

Pseudo $R^2$ is the proportional improvement in the log-likelihood due to the model under consideration compared to the minimal mode:

\begin{equation}
\frac{l(\tilde{\mu}, \phi; y)-l(\hat{\mu}, \phi; y)}{l(\tilde{\mu}, \phi; y)}
\end{equation}

**Example**:
To compute pseudo $R^2$, we need the AIC value of the models. Recall that 

\begin{equation}
AIC=-2l + 2p \Leftrightarrow l = p - \frac{1}{2}AIC
\end{equation}


# Binomial distribution

${n \choose ny} p^{ny} (1-p)^{n-ny},\quad Bi(ny, \frac{\exp[\theta]}{1+\exp[\theta]}) \quad \mu = \frac{\exp[\theta]}{1+\exp[\theta]}$.

## *Logit link

$f(y; \theta_i, \phi)= \exp[\frac{y\theta_i - b(\theta_i)}{\phi/w}+c(y,\phi)]$

\begin{enumerate}
\item $b(\theta)=\log (1+\exp[\theta])$
\begin{enumerate}
\item $b'(\theta)=\mu = \frac{\exp[\theta]}{1+\exp[\theta]}$
\item $b''(\theta)=\mu(1-\mu)$
\end{enumerate}
\item $c(y,\phi)=\log {n \choose ny}$ and $\phi=1, w=n$.
\item The model: $\log[\frac{\mu}{1-\mu}]=\beta_0 + \beta_1x= \eta$, $h(\eta)=\mu\Leftrightarrow \mu = \frac{\exp[\theta]}{1+\exp[\theta]}$ and $g(\mu)=\eta \Leftrightarrow \log[\frac{\mu}{1-\mu}]=\eta$.
\item Mean and Variance: $E(Y)=\mu$, $Var(Y)=b''(\theta)a(\phi)=\frac{\phi}{w} V(\mu)$, where $V(\mu)=\mu(1-\mu)$.
\item Residual deviance:

**Maximal model**: $\mu_i^\diamond=y_i$. 
Recall that:
\begin{equation}
\begin{split}
\ell=& \log f_i(y;\theta_i, \phi)\\
=&  \log \exp [ w_i \frac{y\theta_i - b(\theta_i)}{\phi} + c(y,\phi) ]\\
=& w_i \frac{y\theta_i - b(\theta_i)}{\phi} + c(y,\phi)\\
\end{split}
\end{equation}

For the maximal model:

\begin{equation}
\begin{split}
\ell(y;\theta_i^\diamond, \phi)=& w_i \frac{y\theta_i^\diamond - b(\theta_i^\diamond)}{\phi} + c(y,\phi)\\
\end{split}
\end{equation}

For the model under consideration:

\begin{equation}
\begin{split}
\ell(y;\hat{\theta}_i, \phi)=& w_i \frac{y\hat{\theta}_i - b(\hat{\theta}_i)}{\phi} + c(y,\phi)\\
\end{split}
\end{equation}

Then, scaled deviance for a model $\mu_i = h(x_i^T \beta)$ is defined as:

\small
\begin{equation}
\begin{split}
S(y,\hat{\mu}) =& 
-2 [\ell(\hat{\mu},\phi,y) -  \ell(\hat{\mu}^{\diamond},\phi,y) ]\\
&= 2\sum_i[\frac{w_i}{\phi}[y (\theta_i^\diamond-\hat{\theta}_i)] - b(\theta_i^\diamond) + b(\hat{\theta}_i)  ]\\
\end{split}
\end{equation}

\normalsize
Note that $\phi S(y,\hat{\mu})$ depends only on the data (including $w_i$). This is called residual deviance or deviance.

\begin{equation}
D(y,\hat{\mu})= \phi S(y,\hat{\mu}) = 
2\sum_i w_i[y (\theta_i^\diamond-\hat{\theta}_i) - b(\theta_i^\diamond) + b(\hat{\theta}_i)  ]
\end{equation}

Asymptotically, $S(y,\hat{\mu})$ has a $\chi^2_{n-p}$ distribution. 
\item Pearson residuals:

These are approx.\ $N(0,1)$.

\begin{equation}
\begin{split}
  e_{P,i}=& \sqrt{w_i}\frac{y_i - \hat{\mu}_i}{\sqrt{V(\hat{\mu}_i)}} \\
  =& \frac{y_i - \hat{\mu}_i}{\sqrt{V(\hat{\mu}_i)/w_i}}= \frac{y_i - \hat{\mu}_i}{
  \sqrt{Var(Y_i)/\phi}}
\end{split}  
\end{equation}

\begin{enumerate}
\item Pearson chi-sq statistic: 
\begin{equation}
X^2 = \sum e^2_{P,i}   
\end{equation}

This is asymptotically equivalent to the deviance (D) for a model.
\end{enumerate}
\item Deviance residuals
For the binomial distribution: Deviance $D=\sum d_i$, where:

\begin{equation}
d_i = -2 \times n_i [ y_i \log(\frac{\hat{\mu}_i}{y_i}) + (1-y_i) \log (\frac{1-\hat{\mu}_i}{1-y_i}) ]  
\end{equation}

The $i$-th deviance residual is:

\begin{equation}
	e_{D,i}= sgn(y_i-\hat{\mu}_i) \times \sqrt{d_i}
\end{equation}

Note that $\sum e_{D,i} = D$.

\item Log odds and odds ratio
\begin{enumerate}
\item
log odds: $\lambda=\log[\frac{\mu}{1-\mu}]$. 

\begin{equation}
\begin{split}
Var(\log \lambda) =& \frac{1}{n\mu}+\frac{1}{n(1-\mu)}\\
est.\ Var(\log \hat{\lambda})=& \frac{1}{s}+\frac{1}{n-s}\\
\end{split}
\end{equation}

**Example**: Beetle dataset.

```{r}
head(beetle)
fm1<-glm(propn.dead~conc,
         binomial(logit),
         weights=number,
         data=beetle)
#summary(first.beetle.glm)

## compute log odds of death for 
## concentration 1.7552:
x<-as.matrix(c(1, 1.7552))
#log odds:
(log.odds<-t(x)%*%coef(fm1))

### compute CI for log odds:
## Get vcov matrix:
(vcovmat<-vcov(fm1))
## x^T VCOV x:
(var.log.odds<-t(x)%*%vcovmat%*%x)
##lower
#log.odds-1.96*sqrt(var.log.odds)
##upper
#log.odds+1.96*sqrt(var.log.odds)

## variance of log odds using 
## formula, does not match up
## because it's based only 
## one data point:
(1/18) + (1/(62-18))
```

\item
**Odds ratio**: 
Given: 

\begin{equation}
\frac{p(Y_i=1)}{1-p(Y_i=1)} = \frac{\mu_i}{1-\mu_i}
\end{equation}

\noindent
taking logs:

\begin{equation}
\log\frac{\mu_i}{1-\mu_i}= \alpha + \beta x_i
\end{equation}


Therefore the odds of Y=1 are:

\begin{equation}
\frac{p(Y_i=1)}{1-p(Y_i=1)} = \exp[ \alpha + \beta x_i]
\end{equation}



\end{enumerate}

**Computing odds ratios by hand**:

Odds= no.\ successes / no.\ failures

**Odds ratio**:

Odds(Vaccine group gets Flu) / Odds(Control group gets Flu)

\begin{tabular}{c|cc|c}
 & A1 & A2 & Totals\\
 \hline
B1 & w & x&  w+x\\
B2 & y & z  & y+z \\
\hline
 & w+y  & x+z & \\
\hline
\end{tabular}

**Odds ratio (OR)**: 

\begin{equation}
\frac{w/x}{y/z}
\end{equation}

**CIs for log(OR)**: 

\begin{equation}
\log(OR) \pm 1.96 \times se(\log(OR))
\end{equation}

where

\begin{equation}
 se(\log(OR)) = \sqrt{\frac{1}{w}+\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}
\end{equation}

To get CIs at odds scale just take exponents.

\item Over-dispersion: 



\begin{equation}
\hat{\phi}= \frac{D}{n-p}\approx X^2/(n-p)
\end{equation}

In binomial data,
if observations $Y_i$ have variance greater than that expected from the binomial theorem, then you need to adjust the variance estimate. If the deviance D is greater than N-p (as expected from the fact that it has an approximate chi-squared deviation and the expectation of a chi-sq distributed random variable is N-p; I think), we should suspect that we have an overdispersion problem. Also, correlated binary responses also lead to overdispersion.

Here, we assume that $Var(Y_i)=\phi \frac{\mu_i(1-\mu_i)}{n_i}$.

Once the dispersion parameter (e.g., 3.6185, in an example in lecture notes) has been estimated, we adjust the variance:

\begin{equation*}
Var(Y_i)=\phi \frac{\mu_i(1-\mu_i)}{n_i}=3.6185 \frac{\mu_i(1-\mu_i)}{n_i}
\end{equation*}

I.e., standard errors for the coefficients in the quasibinomial are the result of multiplying the regular SE with the square root of the dispersion parameter estimated.


\end{enumerate}

# Poisson

Let the random variable $X$ count the number of events occurring in the interval. Then under certain reasonable conditions it can be shown that

  \begin{equation}
  f_{X}(x)=\mathbb{P}(X=x)=\mathrm{e}^{-\mu}\frac{\mu^{x}}{x!},\quad x=0,1,2,\ldots
	\end{equation}


In GLM setting, there are two situations where we use the Poisson function:

\begin{enumerate}
\item **Poisson regression**: The events depend on varying amounts of exposure. Predictors can be categorical or continuous.
\item **Log-linear models**: Exposure is constant. Predictors are usually categorical. 
\end{enumerate}

$f(y; \theta_i, \phi)= \exp[\frac{y\theta_i - b(\theta_i)}{\phi/w}+c(y,\phi)]$

\begin{enumerate}
\item $b(\theta)=\exp[\theta]$
\begin{enumerate}
\item $b'(\theta)=\mu = \exp[\theta]$
\item $b''(\theta)=\exp[\theta]$
\end{enumerate}
\item $c(y,\phi)=-\log y!$ and $\phi=1, w=n$.
\item The model: $\log \mu = \beta_0 + \beta_1x=\eta$, $h(\eta)=\mu 
\Leftrightarrow \mu = \exp[\eta]$ and $g(\mu) = \eta \Leftrightarrow 
\log \mu = \eta$.
\item Mean and Variance: $E(Y)=\mu$, $Var(Y)=b''(\theta)a(\phi)=\frac{\phi}{w} V(\mu)$, where $V(\mu)=\mu$.

If $Y_i$ are independent RVs, each denoting the number of events observed from exposure $n_i$ (example: numbers of smoking doctors in each age group). 

**Offset**: Let $E(Y_i)= \mu_i = n_i \theta_i$. Here, $Y_{-i}$ is a count, and $\theta_i$ a function of the predictors $X\beta$: $\theta_i = \exp[x_i^t \beta]$.

Therefore, the GLM is:

\begin{equation}
E(Y_i)= \mu_i = n_i  \exp[x_i^t \beta] \quad Y_i \sim Po(\mu_i)
\end{equation}

The link function is:

\begin{equation}
\log \mu_i = \log n_i + x_i^t \beta
\end{equation}

\noindent
where
$\log n_i$: offset. 

**Fitted values**:
$\hat{Y}_i=\hat{\mu}_i = n_i \exp[x_iT\beta]=e_i$, where $e_i$ refers to **expected value** for $i$.

Since $Var(Y_i)=\mu$, $SE(Y_i)=\sqrt{e_i}$.

\begin{enumerate}
\item
**Pearson residuals**: $r_i=\frac{o_i-e_i}{\sqrt{e_i}}$.
\item
**Chi-squared statistic and $r_i$**: $X^2=\sum r_i^2 = \sum \frac{(o_i-e_i)^2}{e_i}$. 
\item
**Deviance**: $D=2\sum[o_i \log(o_i/e_i)-(o_i-e_i)]
$; note that ``for most models'' $\sum o_i=\sum e_i$, so the last two terms cancel out.
\item
**Deviance residuals**: $d_i = sign(o_i-e_i)\sqrt{2o_i \log(o_i/e_i)-(o_i-e_i)} \rightarrow D=\sum d_i^2$.
\item
**Likelihood ratio chi-sq statistic**: $2[l_{current}-l_{min}]$.
\item
**Pseudo $R^2$**: $\frac{l_{min}-l_{current}}{l_{min}}$.
\item
**Rate ratio**: $\exp[\beta_i]$. Shows, for example, that the risk of coronary death (example below) is $\exp[\beta_i]$ times higher for smokers vs non-smokers, controlling for other factors.
\end{enumerate}
\end{enumerate}

# Contingency tables

## A+B

This is the standard chi-squared analysis, so use that for $\hat{\mu}$:

\begin{equation}
\hat{\mu}=e_{jk}=(y_{j\cdot}y_{\cdot k})/n \quad (row\times col)/total
\end{equation}

\begin{equation}
X^2 = \sum_{jk}\frac{(y_{jk}-e_{jk})^2}{e_{jk}} \quad \chi^2_{(J-1)(K-1)}
\end{equation}

For three-way tables:

\begin{enumerate}
\item Ignore C, compute sums for A, B levels, make a two-way table.
\item Compute fitted values for the above two-way table and then partition the values equally to the two levels of C.
\end{enumerate}

## A+B*C

Algorithm: 
\begin{enumerate}
\item Ignore A, compute sums of 
\begin{verbatim}
B1            B2 
  C1            C1   
  C2            C2
\end{verbatim}
\item Then compute A sums: A1, A2.
\item Use probability p=A1/(A1+A2) and 1-p to multiply with sums of step 1 to get fitted values.
\end{enumerate}

## (A+B)*C

Let A be the response.

\begin{enumerate}
\item 
For each level of C, find the proportions of B regardless of A. 
\item Then multiply the proportions by the row sums of the A levels. 
\end{enumerate}

## Families checklist

\begin{enumerate}
\item
The **gaussian family**: identity, log and inverse.
\item
The **binomial family**: logit, probit, cauchit (Cauchy CDFs) log and cloglog (complementary log-log).
\item
The **Gamma family**: inverse, identity and log.
\item
The **Poisson family**: log, identity, and sqrt.
\item
The **inverse.gaussian** family: 1/mu\^2, inverse, identity and log.
\item
The **quasi family**: logit, probit, cloglog, identity, inverse, log, 1/mu\^2 and sqrt, and the function power can be used to create a power link function.
\end{enumerate}

\begin{verbatim}
binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
## the overdispersion models:
quasi(link = "identity", variance = "constant")
## or variance = "mu"
quasibinomial(link = "logit")
quasipoisson(link = "log")
\end{verbatim}

# Contingency table examples

\begin{verbatim}
           M           F
       N     D      N     D
Y      2     22     4     6      
N      8      2    11     2
\end{verbatim}

```{r}
counts<-c(2,22,4,6,8,2,11,2)
G<-factor(rep(c("M","F"),each=2,2))
R<-factor(rep(c("Y","N"),each=4))
T<-factor(rep(c("N","D"),4))
m1<-glm(counts~G*T,family=poisson)


#fitted(m1)
m2<-glm(counts~G*T+R,family=poisson)
#fitted(m2)
m3<-glm(counts~(T+R)*G,family=poisson)
#fitted(m3)

m4<-glm(counts~(G+R)*T,family=poisson)
fitted(m4)
```

\end{multicols}

\includegraphics{glmdistributions}

```{r echo=F}
y<-seq(0,1,by=0.001)
g.logit<-function(y){
  log(y/(1-y))
}
g.cloglog<-function(y){
  log(-log(1-y))
}
g.probit<-function(y){
  qnorm(y)
}
g.log<-function(y){
  log(y)
}
```

```{r fig=F,echo=F}
op<-par(mfrow=c(2,2),pty="s")

plot(g.logit(y),y,main="logit")
plot(g.probit(y),y,main="probit")
plot(g.cloglog(y),y,main="cloglog")
plot(g.log(y),y,main="log")
```




\end{document}

